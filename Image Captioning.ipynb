{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "In this Project, we will use Flick8k dataset and try to produce image captions on test data. \n",
    "\n",
    "<h4>Note :</h4>\n",
    "My folder names are diiferent in the dataset, which have not been included. So you have to change the folder names accordingly if you want to see the results.\n",
    "Sorry for inconvinience :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "import re\n",
    "from keras.preprocessing import image\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset :\n",
    "\n",
    "Flickr 8k dataset has approx. 8k images and corresponding to each image, 5 captions are provided.\n",
    "\n",
    "To load images and captions for training, cross-validation and testing purposes, text files have been made which can be read and appropriate actions can be taken.\n",
    "\n",
    "These files are also very helpful in making the data generator for caption bot model, which is an important step for this project as dataset is too large to be loaded in RAM and work with it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./flicker8k-dataset/Flickr8k_text/Flickr8k.token.txt\") as f:\n",
    "    captions = f.read()                                                   # Read the captions file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = captions.split(\"\\n\")[:-1]   # Last string is empty , so we remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {}                              # descriptions -> dictionary in the form of image id: captions\n",
    "\n",
    "for ele in captions:\n",
    "    i_to_c = ele.split('\\t')\n",
    "    img_name = i_to_c[0].split('.')[0]         # Image name\n",
    "    cap = i_to_c[1]\n",
    "    \n",
    "    if descriptions.get(img_name) is None:\n",
    "        descriptions[img_name] = []\n",
    "        \n",
    "    descriptions[img_name].append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(descriptions)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. lower case\n",
    "# 2. remove punctuations\n",
    "# 3. remove words length less than 2 ... because punction removal may lead to residual letters, like s,t etc.\n",
    "\n",
    "\n",
    "def clean_text(sample):\n",
    "    sample = sample.lower()\n",
    "    \n",
    "    sample = re.sub(\"[^a-z]+\", \" \", sample)\n",
    "    \n",
    "    sample = sample.split()\n",
    "    \n",
    "    sample = [s for s in sample if len(s)>1] # list comprehension\n",
    "    \n",
    "    sample = \" \".join(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifiying all captions - cleaned captions\n",
    "# Note : This process takes time, so after it's complete, we need to save the dictionary\n",
    "\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc_list[i] = clean_text(desc_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('descriptions.txt', 'w')                     \n",
    "f.write(str(descriptions))                  # Save the description as a string (.txt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary\n",
    "\n",
    "This is an important step in any language model. Here, we first create a set of words as an initial step, and then we further filter out words depending on their frequency. \n",
    "\n",
    "This Voacb is important, as it will later help us in creating word embeddings using GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding unique vocabulary\n",
    "\n",
    "vocabulary = set()\n",
    "\n",
    "for key in descriptions.keys():\n",
    "    [ vocabulary.update(i.split()) for i in descriptions[key]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"vocabulary size : \" , len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words in description dictionary\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for key in descriptions.keys():\n",
    "    [ all_words.append(i) for des in descriptions[key] for i in des.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total words appearing : \" , len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter excess words from vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ = dict(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort acording to frequency \n",
    "\n",
    "sorted_dic = sorted(dic_.items(), key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 10   # Words with frequency less than 10 in the corpus to be discarded\n",
    "\n",
    "d = [x for x in sorted_dic if x[1]>threshold_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [x[0] for x in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Train File\n",
    "\n",
    "with open('flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt') as f:\n",
    "    train = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [e[:-4] for e in train.split('\\n')[:-1]]    # remove .jpg from the image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test File\n",
    "\n",
    "with open('flicker8k-dataset/Flickr8k_text/Flickr_8k.testImages.txt') as f:\n",
    "    test = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [e[:-4] for e in test.split('\\n')[:-1]]    # remove .jpg from the image name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger Words \n",
    "\n",
    "Here, two trigger words startseq and endseq have been added, whose utility will be explained later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = {}                      # train_description -> training dictionary \n",
    "\n",
    "for t in train:\n",
    "    train_descriptions[t] = []\n",
    "    for cap in descriptions[t]:\n",
    "        cap_to_append = \"startseq \" + cap + \" endseq\"\n",
    "        train_descriptions[t].append(cap_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - Images\n",
    "\n",
    "In this project, we will use ResNet50 model and weights for feature extraction and creating feature map and use the second last GAP layer as output i.e encoded image (dim = (2048,))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights = 'imagenet', input_shape = (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Model(inputs = model.input, outputs =  model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):                            \n",
    "    img = image.load_img(img, target_size=(224,224))     # Preprocess input according to ResNet requirements \n",
    "    img = image.img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img):\n",
    "    img = preprocess_image(img)\n",
    "    fea_vec = model_new.predict(img)\n",
    "    fea_vec = fea_vec.reshape(fea_vec.shape[1], )\n",
    "    return fea_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = \"./flicker8k-dataset/Flickr8k_Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : This process takes about 30min , depending on the PC . \n",
    "# So saving the file after the process is important.\n",
    "\n",
    "encoding_train = {}\n",
    "\n",
    "for ix, img in enumerate(train):\n",
    "    \n",
    "    img = images+train[ix]+\".jpg\"\n",
    "    \n",
    "    p = encode_image(img)\n",
    "    \n",
    "    encoding_train[img[len(images):]] = p\n",
    "    \n",
    "    \n",
    "    if ix%100 == 0:\n",
    "        print(\"Encoding image :\" + str(ix))          # Printing after every 100th image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : This process takes about 15min , depending on the PC . \n",
    "# So saving the file after the process is important.\n",
    "\n",
    "encoding_test = {}\n",
    "\n",
    "for ix, img in enumerate(test):\n",
    "    \n",
    "    img = images+test[ix]+\".jpg\"\n",
    "    \n",
    "    p = encode_image(img)\n",
    "    \n",
    "    encoding_test[ img[len(images):] ] = p\n",
    "    \n",
    "    \n",
    "    if ix%100 == 0:\n",
    "        print(\"Encoding image :\" + str(ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the files as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving/dumping features to disk\n",
    "\n",
    "with open(\"./encoded_train_images.pkl\", 'wb') as f:\n",
    "    pickle.dump(encoding_train, f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pickle file\n",
    "\n",
    "with open(\"./encoded_train_images.pkl\", 'rb') as f:\n",
    "    encoding_train = pickle.load(f)\n",
    "\n",
    "    \n",
    "with open(\"./encoded_test_images.pkl\", 'rb') as f:\n",
    "    encoding_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}              \n",
    "idx_to_word = {}\n",
    "\n",
    "ix = 1\n",
    "\n",
    "for e in filtered_words:\n",
    "    word_to_idx[e] = ix\n",
    "    idx_to_word[ix] = e\n",
    "    \n",
    "    ix +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx['startseq'] = 1846\n",
    "word_to_idx['endseq'] = 1847\n",
    "\n",
    "idx_to_word[1846] = 'startseq'        # Add trigger word 'startseq'\n",
    "idx_to_word[1847] = 'endseq'          # Add trigger word 'endseq'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx_to_word) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_caption_len = []\n",
    "\n",
    "for key in train_descriptions.keys():\n",
    "    for cap in train_descriptions[key]:\n",
    "        all_caption_len.append(len(cap.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(all_caption_len)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Last two cells are used to find the max lengh of the senetence .... which is useful for padding the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Function\n",
    "\n",
    "As mentioned earlier, that a generator function is required due to large file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_descriptions, encoding_train, word_to_idx, max_len, batch_size):\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    n=0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for key, desc_list in train_descriptions.items():\n",
    "            n+=1\n",
    "            \n",
    "            photo = encoding_train[key+'.jpg']\n",
    "            \n",
    "            for desc in desc_list:\n",
    "                \n",
    "                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n",
    "                \n",
    "                \n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq = seq[0:i]\n",
    "                    out_seq = seq[i]\n",
    "                    \n",
    "                    in_seq = pad_sequences( [in_seq], maxlen=max_len, value= 0, padding='post')[0]\n",
    "                \n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "                    \n",
    "            if n%batch_size == 0 :\n",
    "                yield [[np.array(X1), np.array(X2)] , np.array(y) ]   #Yield instead of return \n",
    "                X1, X2, y = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_generator(train_descriptions, encoding_train, word_to_idx, max_len, 3):\n",
    "    X, y = i\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "\n",
    "with open('./GloVE/glove.6B.50d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.array(values[1:], dtype=\"float32\")\n",
    "        \n",
    "        embeddings[word] = coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputEmbeddings():\n",
    "\n",
    "    emb_dim = 50\n",
    "    embedding_matrix_output = np.zeros((vocab_size, emb_dim ))\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        \n",
    "        emb_vec = embeddings.get(word)\n",
    "        \n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix_output[idx] = emb_vec\n",
    "            \n",
    "    return embedding_matrix_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output = getOutputEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  image feature extractor model\n",
    "\n",
    "input_img_feat = Input(shape=(2048,))\n",
    "inp_img1 = Dropout(0.3)(input_img_feat)\n",
    "inp_img2 = Dense(256, activation='relu')(inp_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial caption sequence model\n",
    "\n",
    "input_cap = Input(shape=(max_len,))\n",
    "inp_cap1 = Embedding(input_dim= vocab_size, output_dim=50, mask_zero=True)(input_cap)\n",
    "inp_cap2 = Dropout(0.3)(inp_cap1)\n",
    "inp_cap3 = LSTM(256)(inp_cap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = add([inp_img2, inp_cap3])\n",
    "\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "output = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "\n",
    "model = Model(inputs = [input_img_fea, input_cap]  , outputs =  output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_output])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(train_descriptions)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, batch_size)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_weights/best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(photo_enc):\n",
    "    in_text = \"startseq\"\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        sequence = [word_to_idx[word] for word in in_text.split() if word in word_to_idx]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n",
    "        \n",
    "        y_pred = model.predict([photo_enc, sequence])\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        word = idx_to_word[y_pred]\n",
    "        \n",
    "        in_text += \" \"+word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "        \n",
    "        \n",
    "    final_caption = in_text.split()\n",
    "    final_caption = final_caption[1:-1]\n",
    "    final_caption = \" \".join(final_caption)\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn = np.random.randint(0,1000)\n",
    "img_id = list(encoding_test.keys())[rn]             # Random image to be tested\n",
    "\n",
    "photo_enc = encoding_test[img_id].reshape((1,2048))\n",
    "pred = predict(photo_enc)\n",
    "print(pred)\n",
    "\n",
    "path = images + img_id\n",
    "img = plt.imread(path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
